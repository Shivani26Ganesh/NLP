{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9u9ygS9NxrS",
        "outputId": "9ab13c0d-ede5-4d97-b74b-cfcc366ff9d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PreProcessing**"
      ],
      "metadata": {
        "id": "ZrDh90ZgOFWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import ne_chunk,pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import CFG\n",
        "\n",
        "#tokenization\n",
        "text = \"Hello! How are you doing today? I'm learning NLTK.\"\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word tokens: \",word_tokens)\n",
        "\n",
        "#get synonyms\n",
        "synonyms = wordnet.synsets('good')\n",
        "print(synonyms)\n",
        "print(\"Examples : \", synonyms[0].examples())\n",
        "\n",
        "#stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "print(\"Filtered words:\",filtered_tokens)\n",
        "\n",
        "#stemming\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in filtered_tokens]\n",
        "print(\"Stmemed words : \",stemmed_words)\n",
        "\n",
        "#lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print([lemmatizer.lemmatize(word) for word in filtered_tokens])\n",
        "\n",
        "#named entity recognition\n",
        "pos_tags = pos_tag(word_tokens)\n",
        "entities=ne_chunk(pos_tags)\n",
        "print(\"Entites :\")\n",
        "print(entities)\n",
        "\n",
        "#parse tree sentence\n",
        "grammar = CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "NP -> Det N\n",
        "VP -> V PP\n",
        "PP -> P NP\n",
        "Det -> 'the'\n",
        "N -> 'cat' | 'mat'\n",
        "V -> 'sat'\n",
        "P -> 'on'\n",
        "\"\"\")\n",
        "parser = nltk.ChartParser(grammar)\n",
        "sentence = \"the cat sat on the mat\".split()\n",
        "for tree in parser.parse(sentence):\n",
        "  tree.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgZDgNyONzIl",
        "outputId": "60f5ccdb-4bb3-4c7e-f94d-125cb9c3f155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokens:  ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'I', \"'m\", 'learning', 'NLTK', '.']\n",
            "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
            "Examples :  ['for your own good', \"what's the good of worrying?\"]\n",
            "Filtered words: ['Hello', '!', 'today', '?', \"'m\", 'learning', 'NLTK', '.']\n",
            "Stmemed words :  ['hello', '!', 'today', '?', \"'m\", 'learn', 'nltk', '.']\n",
            "['Hello', '!', 'today', '?', \"'m\", 'learning', 'NLTK', '.']\n",
            "Entites :\n",
            "(S\n",
            "  (GPE Hello/NN)\n",
            "  !/.\n",
            "  How/WRB\n",
            "  are/VBP\n",
            "  you/PRP\n",
            "  doing/VBG\n",
            "  today/NN\n",
            "  ?/.\n",
            "  I/PRP\n",
            "  'm/VBP\n",
            "  learning/VBG\n",
            "  (ORGANIZATION NLTK/NNP)\n",
            "  ./.)\n",
            "             S                     \n",
            "      _______|_______               \n",
            "     |               VP            \n",
            "     |        _______|___           \n",
            "     |       |           PP        \n",
            "     |       |    _______|___       \n",
            "     NP      |   |           NP    \n",
            "  ___|___    |   |        ___|___   \n",
            "Det      N   V   P      Det      N \n",
            " |       |   |   |       |       |  \n",
            "the     cat sat  on     the     mat\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-test\n"
      ],
      "metadata": {
        "id": "xwUD9EIEcdZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from nltk import word_tokenize\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "\n",
        "# Function to calculate the t-statistic for bigrams\n",
        "def t_test_for_bigrams(w1, w2, sample_text, n):\n",
        "    n1, n2 = sample_text.count(w1), sample_text.count(w2)  # Word frequencies\n",
        "    O = sample_text.count(f\"{w1} {w2}\")  # Observed frequency of bigram\n",
        "    E = (n1 * n2) / n  # Expected frequency of bigram\n",
        "    return (O - E) / (E / math.sqrt(n)) if E > 0 else 0  # Calculate t-statistic\n",
        "\n",
        "# Read the text\n",
        "with open(\"/content/ab.txt\", \"r\") as file:\n",
        "    sample_text = file.read()\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sample_text.lower())\n",
        "n = len(tokens)  # Total number of tokens\n",
        "\n",
        "# Find bigram collocations based on PMI (Pointwise Mutual Information)\n",
        "finder = BigramCollocationFinder.from_words(tokens)\n",
        "collocations = finder.nbest(BigramAssocMeasures.pmi, 10)\n",
        "\n",
        "# Get critical value and check for collocations\n",
        "cv = float(input(\"Enter critical value: \"))\n",
        "print(\"Collocations (with t-statistic > critical value):\")\n",
        "for w1, w2 in collocations:\n",
        "    t_stat = t_test_for_bigrams(w1, w2, sample_text, n)\n",
        "    if t_stat > cv:  # If t-statistic exceeds critical value, it's a collocation\n",
        "        print(f\"Collocation: {w1} {w2}\")\n",
        "    else:\n",
        "      print(\"Not a collocation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "n2IsaxexLzEz",
        "outputId": "da125942-9957-4cda-81d6-743ccffa365b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/ab.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-eb8b9b8463d2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Read the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/ab.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/ab.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chi2"
      ],
      "metadata": {
        "id": "3xh9h1COcgL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from nltk import word_tokenize\n",
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Function to calculate the Chi-Square statistic for bigrams\n",
        "def chi_square_for_bigrams(w1, w2, sample_text, n):\n",
        "    n1, n2 = sample_text.count(w1), sample_text.count(w2)  # Word frequencies\n",
        "    O = sample_text.count(f\"{w1} {w2}\")  # Observed frequency of bigram\n",
        "    E = (n1 * n2) / n  # Expected frequency of bigram\n",
        "\n",
        "    # Chi-Square formula: Chi2 = sum((O - E)^2 / E)\n",
        "    chi_square = ((O - E) ** 2) / E if E > 0 else 0\n",
        "    return chi_square  # Return the calculated Chi-Square value\n",
        "\n",
        "# Read the text\n",
        "with open(\"/content/ab.txt\", \"r\") as file:\n",
        "    sample_text = file.read()\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(sample_text.lower())\n",
        "n = len(tokens)  # Total number of tokens\n",
        "\n",
        "# Find bigram collocations based on PMI (Pointwise Mutual Information)\n",
        "finder = BigramCollocationFinder.from_words(tokens)\n",
        "collocations = finder.nbest(BigramAssocMeasures.pmi, 10)\n",
        "\n",
        "# Get critical value and check for collocations\n",
        "cv = float(input(\"Enter critical value: \"))\n",
        "\n",
        "for w1, w2 in collocations:\n",
        "    chi_square_stat = chi_square_for_bigrams(w1, w2, sample_text, n)\n",
        "    if chi_square_stat > cv:  # If Chi-Square statistic exceeds critical value, it's a collocation\n",
        "        print(f\"Collocation: {w1} {w2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAgn0YMRcCHo",
        "outputId": "7d22718e-5a03-49a7-d520-99fe6c483d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter critical value: 2\n",
            "Collocation: 'd tell\n",
            "Collocation: an expert\n",
            "Collocation: another city\n",
            "Collocation: balance between\n",
            "Collocation: barista named\n",
            "Collocation: bean origins\n",
            "Collocation: began noticing\n",
            "Collocation: between readability\n",
            "Collocation: can enjoy\n",
            "Collocation: care package\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "count=int(input(\"Enter the number of varients senses: \"))\n",
        "sense_word_pair={}\n",
        "sense_count={}\n",
        "while(count):\n",
        "  sense= input(\"Sense: \")\n",
        "  sense_word_pair[sense]=[]\n",
        "  n=int(input(f\"enter nuber of sentences assigned-{sense} :\"))\n",
        "  sense_count[sense]=n\n",
        "  while(n):\n",
        "    sentence=input(\"enter the sentence: \")\n",
        "    tokenized_sentence=word_tokenize(sentence)\n",
        "    stopword=set(stopwords.words(\"english\"))\n",
        "    for j in tokenized_sentence:\n",
        "      if j not in stopword and j not in [\",\",\".\",\"(\",\")\",\"?\"] and j.isalpha():\n",
        "        sense_word_pair[sense].append(j)\n",
        "    n-=1\n",
        "  count-=1\n",
        "print(sense_word_pair.items(),end=\"\\n\\n\")\n",
        "print(sense_count.items())\n",
        "wordpair_count={}\n",
        "for sense,word in sense_word_pair.items():\n",
        "  counter=Counter(word)\n",
        "  wordpair_count[sense]=counter\n",
        "print(wordpair_count.items())\n",
        "vocabulary=0\n",
        "for i in wordpair_count:\n",
        "  vocabulary+=len(wordpair_count[i])\n",
        "print(vocabulary)\n",
        "wordpair_probability={}\n",
        "for sense,words in wordpair_count.items():\n",
        "  for word,count in words.items():\n",
        "    wordpair_probability[(word,sense)]=(wordpair_count[sense][word]+1)/(vocabulary+sense_count[sense])\n",
        "print(wordpair_probability.items())\n",
        "test_sentence=input(\"Enter sentence to find sense of it\")\n",
        "sentence_token=word_tokenize(test_sentence)\n",
        "removed_stopword=[]\n",
        "for j in sentence_token:\n",
        "  if j not in stopword and j not in [\",\",\".\",\"(\",\")\",\"?\"] and j.isalpha():\n",
        "    removed_stopword.append(j)\n",
        "test_probability={}\n",
        "for sense in sense_count:\n",
        "  prob_value=math.log2(sense_count[sense]/sum(sense_count.values()))\n",
        "  for word in removed_stopword:\n",
        "    if (word,sense) not in wordpair_probability.keys():\n",
        "      prob_value+=math.log2(1/(vocabulary+sense_count[sense]))\n",
        "    else:\n",
        "      prob_value+=math.log2(wordpair_probability[(word,sense)])\n",
        "  test_probability[prob_value]=sense\n",
        "print(f\"Sense of \\\"{test_sentence}\\\"is\",test_probabili2ty[max(test_probability.keys())])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsD9hyvqcZAU",
        "outputId": "361f822b-c4a5-4800-d5ff-2af0350a8a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of varients senses: 2\n",
            "Sense: dog\n",
            "enter nuber of sentences assigned-dog :2\n",
            "enter the sentence: the dog barks at stranger\n",
            "enter the sentence: dog wags its tail\n",
            "Sense: cat\n",
            "enter nuber of sentences assigned-cat :2\n",
            "enter the sentence: cat scratches the stranger face\n",
            "enter the sentence: cat drinks milk\n",
            "dict_items([('dog', ['dog', 'barks', 'stranger', 'dog', 'wags', 'tail']), ('cat', ['cat', 'scratches', 'stranger', 'face', 'cat', 'drinks', 'milk'])])\n",
            "\n",
            "dict_items([('dog', 2), ('cat', 2)])\n",
            "dict_items([('dog', Counter({'dog': 2, 'barks': 1, 'stranger': 1, 'wags': 1, 'tail': 1})), ('cat', Counter({'cat': 2, 'scratches': 1, 'stranger': 1, 'face': 1, 'drinks': 1, 'milk': 1}))])\n",
            "11\n",
            "dict_items([(('dog', 'dog'), 0.23076923076923078), (('barks', 'dog'), 0.15384615384615385), (('stranger', 'dog'), 0.15384615384615385), (('wags', 'dog'), 0.15384615384615385), (('tail', 'dog'), 0.15384615384615385), (('cat', 'cat'), 0.23076923076923078), (('scratches', 'cat'), 0.15384615384615385), (('stranger', 'cat'), 0.15384615384615385), (('face', 'cat'), 0.15384615384615385), (('drinks', 'cat'), 0.15384615384615385), (('milk', 'cat'), 0.15384615384615385)])\n",
            "Enter sentence to find sense of itdog scratches stranger face\n",
            "Sense of \"dog scratches stranger face\"is cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes\n"
      ],
      "metadata": {
        "id": "LAZ-vd1Nfbhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Sample labeled sentences\n",
        "data = [\n",
        "    (\"He went to the bank to deposit money\", \"financial\"),\n",
        "    (\"The bank loaned him money to buy a house\", \"financial\"),\n",
        "    (\"She saved her money in the bank account\", \"financial\"),\n",
        "    (\"He stood by the river bank and watched the water flow\", \"river\"),\n",
        "    (\"They went fishing by the bank of the river\", \"river\"),\n",
        "    (\"The river bank was muddy after the rain\", \"river\"),\n",
        "]\n",
        "\n",
        "# Frequency counts for words and classes\n",
        "word_counts = defaultdict(lambda: defaultdict(int))\n",
        "class_counts = defaultdict(int)\n",
        "for sentence, label in data:\n",
        "    class_counts[label] += 1\n",
        "    for word in sentence.lower().split():\n",
        "        word_counts[label][word] += 1\n",
        "\n",
        "# Classify function using Naive Bayes with Laplace smoothing\n",
        "def classify(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    scores = {}\n",
        "    for label in class_counts:\n",
        "        log_prob = math.log(class_counts[label] / len(data))  # prior\n",
        "        for word in words:\n",
        "            word_prob = (word_counts[label][word] + 1) / (sum(word_counts[label].values()) + len(word_counts[label]))\n",
        "            log_prob += math.log(word_prob)\n",
        "        scores[label] = log_prob\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "# Testing the function\n",
        "test_sentence = \"The bank is by the river and full of fish\"\n",
        "print(\"Predicted sense:\", classify(test_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpcBxWDrWTN6",
        "outputId": "2eb7351f-9679-41da-e4f1-1905f14b3205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sense: river\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hindle and rooth"
      ],
      "metadata": {
        "id": "7A72dDfYgYWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def train(corpus):\n",
        "  counts=defaultdict(int)\n",
        "  for sentence in corpus:\n",
        "    for i in range(len(sentence)-2):\n",
        "      counts[(sentence[i].lower(),sentence[i+1].lower(),sentence[i+2].lower())]+=1\n",
        "  return counts\n",
        "def hindle_and_rooth(word1, preposition, word2,counts):\n",
        "  trigram1 =(word1.lower(), preposition.lower(), word2.lower())\n",
        "  trigram2 = (word2.lower(), preposition.lower(), word1.lower())\n",
        "  if trigram1 in counts:\n",
        "    return \"Preposition \"+preposition+\" attached with \"+word1\n",
        "  elif trigram2 in counts:\n",
        "    return \"Preposition \"+preposition+\" attached with \"+ word2\n",
        "  else:\n",
        "    return \"No attachment\"\n",
        "corpus=[['The','girl','with','the','curly','hairs','is','eating','at','home']]\n",
        "word1=\"eating\"\n",
        "word2=\"home\"\n",
        "preposition=\"at\"\n",
        "counts=train(corpus)\n",
        "print(hindle_and_rooth(word1,preposition,word2,counts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0f6SWCIfgk5",
        "outputId": "bd235a7a-6bc1-48c7-fe7c-45ce694e9674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preposition at attached with eating\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HMM"
      ],
      "metadata": {
        "id": "pwRwzLHUhAra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def forward_backward(observations, states, initial_probs, transition_matrix, emission_matrix):\n",
        "    T = len(observations)\n",
        "    N = len(states)\n",
        "\n",
        "    # Forward algorithm\n",
        "    alpha = np.zeros((T, N))\n",
        "    alpha[0] = initial_probs# * emission_matrix[:, observations[0]]\n",
        "    print(alpha[0])\n",
        "    for t in range(1, T):\n",
        "        for j in range(N):\n",
        "            alpha[t, j] = np.sum(alpha[t-1] * transition_matrix[:, j] * emission_matrix[j, observations[t]])\n",
        "\n",
        "    # Backward algorithm\n",
        "    beta = np.zeros((T, N))\n",
        "    beta[T-1] = np.ones(N)\n",
        "    for t in range(T-2, -1, -1):\n",
        "        for i in range(N):\n",
        "            beta[t, i] = np.sum(transition_matrix[i] * emission_matrix[:, observations[t+1]] * beta[t+1])\n",
        "\n",
        "    return alpha, beta\n",
        "\n",
        "# Example usage:\n",
        "states = ['Sunny', 'Rainy']\n",
        "observations = [\"walk\", \"clean\", \"shop\"]\n",
        "initial_probs = np.array([0.5, 0.5])\n",
        "transition_matrix = np.array([[0.7, 0.3],[0.4, 0.6]])\n",
        "emission_matrix = np.array([[0.1, 0.4, 0.5],\n",
        "                            [0.6, 0.3, 0.1]])\n",
        "\n",
        "obs_indices = [observations.index(obs) for obs in observations]\n",
        "alpha, beta = forward_backward(obs_indices, states, initial_probs, transition_matrix, emission_matrix)\n",
        "\n",
        "print(\"Forward (Alpha) probabilities:\\n\", alpha)\n",
        "print(\"Backward (Beta) probabilities:\\n\", beta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_xggIu0gaze",
        "outputId": "2e7430dc-11df-46ba-8216-c54efe5a370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5 0.5]\n",
            "Forward (Alpha) probabilities:\n",
            " [[0.5    0.5   ]\n",
            " [0.22   0.135 ]\n",
            " [0.104  0.0147]]\n",
            "Backward (Beta) probabilities:\n",
            " [[0.1298 0.1076]\n",
            " [0.38   0.26  ]\n",
            " [1.     1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viterbi HMM"
      ],
      "metadata": {
        "id": "TYATqclxhrjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def viterbi(pi, transition_matrix, emission_matrix, observations):\n",
        "    T = len(observations)\n",
        "    N = len(pi)\n",
        "\n",
        "    # Initialization\n",
        "    viterbi = np.zeros((T, N))\n",
        "    backpointer = np.zeros((T, N), dtype=int)\n",
        "    viterbi[0] = pi #* emission_matrix[:, observations[0]]\n",
        "    backpointer[0] = np.arange(N)\n",
        "    print(backpointer[0])\n",
        "    # Recursion\n",
        "    for t in range(1, T):\n",
        "        for j in range(N):\n",
        "            viterbi[t, j] = np.max(viterbi[t-1] * transition_matrix[:, j] * emission_matrix[j, observations[t]])\n",
        "            backpointer[t, j] = np.argmax(viterbi[t-1] * transition_matrix[:, j] * emission_matrix[j, observations[t]])\n",
        "\n",
        "    # Termination\n",
        "    best_path_prob = np.max(viterbi[T-1])\n",
        "    best_path_pointer = np.argmax(viterbi[T-1])\n",
        "\n",
        "    # Backtracking\n",
        "    best_path = []\n",
        "    for t in range(T-1, -1, -1):\n",
        "        best_path.append(best_path_pointer)\n",
        "        best_path_pointer = backpointer[t, best_path_pointer]\n",
        "    best_path.reverse()\n",
        "\n",
        "    return best_path_prob, best_path\n",
        "\n",
        "# Example usage\n",
        "pi = np.array([0.6, 0.4])\n",
        "transition_matrix = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
        "emission_matrix = np.array([[0.1, 0.4, 0.5], [0.6, 0.3, 0.1]])\n",
        "observations = [2, 1, 0]  # Index-based representation of observations\n",
        "\n",
        "probability, path = viterbi(pi, transition_matrix, emission_matrix, observations)\n",
        "print(\"Probability of the most likely sequence:\", probability)\n",
        "print(\"Most likely sequence of states:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxZ-1P-7hDZa",
        "outputId": "356e0a52-8efa-4fe2-cb15-0124a9f1c445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n",
            "Probability of the most likely sequence: 0.03024\n",
            "Most likely sequence of states: [0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCFG"
      ],
      "metadata": {
        "id": "HZEZZnfFlemp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import PCFG\n",
        "import nltk\n",
        "\n",
        "pcfg_grammar = PCFG.fromstring(\"\"\"\n",
        "S -> NP VP [0.7]\n",
        "S -> N VP [0.3]\n",
        "PP -> P NP [1.0]\n",
        "NP -> Det N [0.3] | Det N PP [0.6] | 'I' [0.1]\n",
        "VP -> V NP [0.25] | VP PP [0.25] | V NP [0.25] | V PP [0.25]\n",
        "Det -> 'an' [0.4] | 'my' [0.3] | 'a' [0.3]\n",
        "N -> 'elephant' [0.4] | 'pajamas' [0.1] | 'I' [0.5]\n",
        "V -> 'shot' [0.3] | 'am' [0.2] | 'is' [0.5]\n",
        "P -> 'in' [0.5] | 'of' [0.5]\n",
        "\"\"\")\n",
        "\n",
        "sentence = ['I', 'shot', 'an', 'elephant']\n",
        "parser = nltk.ChartParser(pcfg_grammar)\n",
        "\n",
        "def calculate_tree_probability(tree, grammar):\n",
        "    probability = 1.0\n",
        "    for prod in tree.productions():\n",
        "        for rule in grammar.productions():\n",
        "            if rule.lhs() == prod.lhs() and rule.rhs() == prod.rhs():\n",
        "                probability *= rule.prob()\n",
        "                break\n",
        "    return probability\n",
        "\n",
        "parses = list(parser.parse(sentence))\n",
        "\n",
        "i = 0\n",
        "for tree in parses:\n",
        "    print(f\"Parse Tree {i+1}:\")\n",
        "    i += 1\n",
        "    print(tree)\n",
        "    prob = calculate_tree_probability(tree, pcfg_grammar)\n",
        "    print(f\"Probability of this tree: {prob}\\n\")\n",
        "\n",
        "total_prob = sum(calculate_tree_probability(tree, pcfg_grammar) for tree in parses)\n",
        "print(f\"Total probability of all trees: {total_prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D--DsLj3huPx",
        "outputId": "d2069451-b412-4419-d8c3-0baf4d2e4bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse Tree 1:\n",
            "(S (N I) (VP (V shot) (NP (Det an) (N elephant))))\n",
            "Probability of this tree: 0.00054\n",
            "\n",
            "Parse Tree 2:\n",
            "(S (NP I) (VP (V shot) (NP (Det an) (N elephant))))\n",
            "Probability of this tree: 0.000252\n",
            "\n",
            "Total probability of all trees: 0.000792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import PCFG, ViterbiParser\n",
        "import nltk\n",
        "\n",
        "# Define the PCFG grammar\n",
        "pcfg_grammar = PCFG.fromstring(\"\"\"\n",
        "S -> NP VP [0.7]\n",
        "S -> N VP [0.3]\n",
        "PP -> P NP [1.0]\n",
        "NP -> Det N [0.3] | Det N PP [0.6] | 'I' [0.1]\n",
        "VP -> V NP [0.25] | VP PP [0.25] | V NP [0.25] | V PP [0.25]\n",
        "Det -> 'an' [0.4] | 'my' [0.3] | 'a' [0.3]\n",
        "N -> 'elephant' [0.4] | 'pajamas' [0.1] | 'I' [0.5]\n",
        "V -> 'shot' [0.3] | 'am' [0.2] | 'is' [0.5]\n",
        "P -> 'in' [0.5] | 'of' [0.5]\n",
        "\"\"\")\n",
        "\n",
        "# Define the sentence to parse\n",
        "sentence = ['I', 'shot', 'an', 'elephant']\n",
        "\n",
        "# Initialize the Viterbi parser with the PCFG grammar\n",
        "viterbi_parser = ViterbiParser(pcfg_grammar)\n",
        "\n",
        "# Parse the sentence to find the most probable parse\n",
        "parses = list(viterbi_parser.parse(sentence))\n",
        "\n",
        "# If there is a parse, print the most probable one and its probability\n",
        "if parses:\n",
        "    best_tree = parses[0]\n",
        "    print(\"Most probable parse tree:\")\n",
        "    print(best_tree)\n",
        "    print(f\"Probability of the most probable parse tree: {best_tree.prob()}\")\n",
        "else:\n",
        "    print(\"No parse found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN_3xXnNKlvB",
        "outputId": "4c1be8dc-1bef-4a86-9747-c0b46e55db8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most probable parse tree:\n",
            "(S (N I) (VP (V shot) (NP (Det an) (N elephant)))) (p=0.00054)\n",
            "Probability of the most probable parse tree: 0.00054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ck7pDsiQKmVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ],
      "metadata": {
        "id": "pl-8sApt8Gj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Sample text data\n",
        "text = \"This is a sample text for word2vec. We will use this text to train our model.\"\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Model (Skip-Gram)\n",
        "model = Sequential([\n",
        "    Embedding(len(word_index) + 1, 100, input_length=1),\n",
        "    Flatten(),\n",
        "    Dense(len(word_index) + 1, activation='softmax')\n",
        "])\n",
        "model.compile('adam', 'sparse_categorical_crossentropy')\n",
        "\n",
        "# Training data\n",
        "window_size = 2\n",
        "data = [(i, j) for i, word in enumerate(word_index)\n",
        "        for j in range(max(0, i - window_size), min(len(word_index), i + window_size + 1))\n",
        "        if i != j]\n",
        "\n",
        "# Train the model\n",
        "x_train, y_train = zip(*data)\n",
        "model.fit(np.array(x_train), np.array(y_train), epochs=5, verbose=1)\n",
        "\n",
        "# Find similar words\n",
        "def find_similar_words(word):\n",
        "    idx = tokenizer.word_index[word]\n",
        "    word_vec = model.layers[0].get_weights()[0][idx - 1]  # Corrected index for 1-based indexing\n",
        "    similarities = np.dot(model.layers[0].get_weights()[0], word_vec) / np.linalg.norm(model.layers[0].get_weights()[0], axis=1)\n",
        "    return [tokenizer.index_word[i + 1] for i in np.argsort(similarities)[::-1][1:6]]  # Corrected to ensure 1-based index access\n",
        "\n",
        "print(find_similar_words('text'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRVg7cgB6l1R",
        "outputId": "376ba6a9-8d1e-4df7-bb80-2980da816806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 2.7166\n",
            "Epoch 2/5\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7034 \n",
            "Epoch 3/5\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6950 \n",
            "Epoch 4/5\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6850 \n",
            "Epoch 5/5\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6765 \n",
            "['our', 'this', 'sample', 'use', 'word2vec']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement word2vec model to explore the semantic similarity between the words.\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "#sentences = ['I love programming', 'Python is great', 'I enjoy machine learning',\n",
        "            # 'TensorFlow is a powerful tool', 'AI is the future']\n",
        "with open('/content/ab.txt', 'r') as f:\n",
        "  sentences=f.read()\n",
        "sentences = sent_tokenize(sentences)  # Tokenize the text into sentences\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]  # Tokenize each sentence\n",
        "\n",
        "\n",
        "#print(tokenized_sentences)\n",
        "model_w2v = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "#  word vectors (semantic similarity)\n",
        "word = 'strong'\n",
        "similar_words = model_w2v.wv.most_similar(word, topn=5)\n",
        "print(f\"Words most similar to '{word}':\")\n",
        "for sim_word, sim_score in similar_words:\n",
        "    print(f\"{sim_word}: {sim_score:.4f}\")\n",
        "#model_w2v.save(\"word2vec_model.bin\")\n",
        "#loaded_model = Word2Vec.load(\"word2vec_model.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bNt4vi5aVD6",
        "outputId": "e0fa067b-2c4a-4504-8092-c7eb34abfe08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'strong':\n",
            "formatted: 0.3225\n",
            "and: 0.3115\n",
            "colleagues: 0.2919\n",
            "their: 0.2623\n",
            "a: 0.2595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf-idf bow\n"
      ],
      "metadata": {
        "id": "EuSViR7ea1Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.preprocessing import *\n",
        "from tensorflow import *\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.models import *\n",
        "data = {'text':['i love programming', 'pythhon is interesting', 'i enjoy machine learning'],\n",
        "        'label':['positive', 'positive','neutral']}\n",
        "df = pd.DataFrame(data)\n",
        "df['label']=LabelEncoder().fit_transform(df['label'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2,random_state=42)\n",
        "def vectorize(vectorizer):\n",
        "  X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
        "  X_test_vec = vectorizer.transform(X_test).toarray()\n",
        "  return X_train_vec, X_test_vec\n",
        "def build(X_train_vec, X_test_vec):\n",
        "    model = Sequential([Dense(16, activation='relu', input_dim=X_train_vec.shape[1]),\n",
        "                        Dense(8, activation='relu'),\n",
        "                        Dense(1, activation='sigmoid')])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_vec, y_train, epochs=10, batch_size=2, verbose=1)\n",
        "    return model.evaluate(X_test_vec, y_test)[1]\n",
        "acc_bow = build(*vectorize(CountVectorizer()))\n",
        "acc_tfidf = build(*vectorize(TfidfVectorizer()))\n",
        "print(\"bow: \",acc_bow)\n",
        "print(\"tfidf: \",acc_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN7RWxhsYGaK",
        "outputId": "10c9d850-5b04-438b-fa69-ec55b2f3078a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6851\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6790\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 0.6730\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5000 - loss: 0.6670\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5000 - loss: 0.6610\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6551\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5000 - loss: 0.6493\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 0.6435\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6377\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5000 - loss: 0.6320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7b33fbed68c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.0000e+00 - loss: 0.7013\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.6341\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6300\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6262\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.6222\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6182\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.6143\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6104\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.6064\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.6025\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.5985\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.0000e+00 - loss: 0.7076\n",
            "bow:  0.0\n",
            "tfidf:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import *\n",
        "from sklearn.preprocessing import *\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "data = {'text': ['I love programming', 'Python is great', 'I enjoy machine learning',\n",
        "                 'TensorFlow is a powerful tool', 'AI is the future'],\n",
        "        'label': ['positive', 'positive', 'positive', 'positive', 'neutral']}\n",
        "df = pd.DataFrame(data)\n",
        "df['label'] = LabelEncoder().fit_transform(df['label'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "def vectorize_data(vectorizer):\n",
        "    X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_vec = vectorizer.transform(X_test).toarray()\n",
        "    return X_train_vec, X_test_vec\n",
        "\n",
        "# Build and train model\n",
        "def build_and_train_model(X_train_vec, X_test_vec):\n",
        "    model = Sequential([Dense(16, activation='relu', input_dim=X_train_vec.shape[1]),\n",
        "                        Dense(8, activation='relu'),\n",
        "                        Dense(1, activation='sigmoid')])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(X_train_vec, y_train, epochs=10, batch_size=2, verbose=1)\n",
        "    return model.evaluate(X_test_vec, y_test)[1]\n",
        "\n",
        "# Evaluate models\n",
        "accuracy_bow = build_and_train_model(*vectorize_data(CountVectorizer()))\n",
        "print(f'BoW Model Accuracy: {accuracy_bow:.2f}')\n",
        "\n",
        "accuracy_tfidf = build_and_train_model(*vectorize_data(TfidfVectorizer()))\n",
        "print(f'TF-IDF Model Accuracy: {accuracy_tfidf:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFmlam50Y11w",
        "outputId": "d181c296-ace6-4851-b968-c72174e0ba1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6667 - loss: 0.6387\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8333 - loss: 0.6310 \n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6667 - loss: 0.6574 \n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8333 - loss: 0.6409 \n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8333 - loss: 0.6361 \n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8333 - loss: 0.6237 \n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8333 - loss: 0.6179 \n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6667 - loss: 0.6157 \n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6667 - loss: 0.6082 \n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.6010\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.6498\n",
            "BoW Model Accuracy: 1.00\n",
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.1667 - loss: 0.7679   \n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3333 - loss: 0.7524 \n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5000 - loss: 0.7523 \n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5000 - loss: 0.7376 \n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5000 - loss: 0.7306 \n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3333 - loss: 0.7496     \n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3333 - loss: 0.7417     \n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5000 - loss: 0.7154 \n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3333 - loss: 0.7293     \n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3333 - loss: 0.7231     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7b33fcc76710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.0000e+00 - loss: 0.7259\n",
            "TF-IDF Model Accuracy: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag Of Words**"
      ],
      "metadata": {
        "id": "DGsWvMIrpQ3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "texts = [\"This is a positive review\", \"I enjoyed the movie a lot\", \"Great performance by the actors\",\n",
        "         \"The plot was intriguing\", \"Negative feedback about the service\", \"Disappointed with the product quality\",\n",
        "         \"Worst experience ever\"]\n",
        "labels = [1, 1, 1, 1, 0, 0, 0]\n",
        "\n",
        "# Vectorize the text\n",
        "X = CountVectorizer().fit_transform(texts).toarray()\n",
        "y = np.array(labels)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build, compile and train the model\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],))])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2)\n",
        "\n",
        "# Evaluate and predict\n",
        "accuracy = model.evaluate(X_test, y_test)[1]\n",
        "predictions = (model.predict(CountVectorizer().fit(texts).transform([\"I loved the movie\", \"Worst product ever\"]).toarray()) > 0.5).astype(int)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa1DMklxolC2",
        "outputId": "f5d50efc-3227-4bee-9cfb-8abdbf57aaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.6217  \n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8375 - loss: 0.6287 \n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7125 - loss: 0.6610 \n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7125 - loss: 0.6479 \n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.6096 \n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.6071 \n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.5985 \n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.6108 \n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.5938 \n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8375 - loss: 0.5947 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: 0.8282\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Test Accuracy: 50.00%\n",
            "Predictions: [[0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF"
      ],
      "metadata": {
        "id": "mLjn4IhjqHmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data\n",
        "texts = [\n",
        "    \"This is a positive review\", \"I enjoyed the movie a lot\", \"Great performance by the actors\",\n",
        "    \"The plot was intriguing\", \"Negative feedback about the service\",\n",
        "    \"Disappointed with the product quality\", \"Worst experience ever\"\n",
        "]\n",
        "labels = [1, 1, 1, 1, 0, 0, 0]\n",
        "\n",
        "# TF-IDF Vectorization and data split\n",
        "X = TfidfVectorizer().fit_transform(texts).toarray()\n",
        "y = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model creation, compilation, and training\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(X_train.shape[1],))])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2)\n",
        "\n",
        "# Evaluation and prediction\n",
        "accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
        "predictions = (model.predict(TfidfVectorizer().fit(texts).transform([\"I loved the movie\", \"Worst product ever\"]).toarray()) > 0.5).astype(int)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOlPhtIhpUPF",
        "outputId": "bba23a84-ef3f-4dbf-eb27-8a16e55fdc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4500 - loss: 0.7047  \n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2625 - loss: 0.7475     \n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4500 - loss: 0.7051 \n",
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5750 - loss: 0.6946 \n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4500 - loss: 0.6944 \n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2625 - loss: 0.7256     \n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4500 - loss: 0.7238 \n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4500 - loss: 0.6926 \n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3250 - loss: 0.7342     \n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2625 - loss: 0.7377     \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Accuracy: 50.00%\n",
            "Predictions: [[1]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation using LSTM**"
      ],
      "metadata": {
        "id": "Hbqt2_4hPwT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"This is a positive review\",\n",
        "    \"I enjoyed the movie a lot\",\n",
        "    \"Great performance by the actors\",\n",
        "    \"The plot was intriguing\",\n",
        "    \"Negative feedback about the service\",\n",
        "    \"Disappointed with the product quality\",\n",
        "    \"Worst experience ever\"\n",
        "]\n",
        "\n",
        "# Tokenize and create sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "sequences = [tokenizer.texts_to_sequences([text])[0] for text in corpus]\n",
        "sequences = [seq[:i+1] for seq in sequences for i in range(1, len(seq))]\n",
        "\n",
        "# Prepare input (X) and output (y)\n",
        "X, y = zip(*[(seq[:-1], seq[-1]) for seq in pad_sequences(sequences, maxlen=max(len(seq) for seq in sequences), padding='pre')])\n",
        "X, y = np.array(X), tf.keras.utils.to_categorical(np.array(y), num_classes=total_words)\n",
        "\n",
        "# Build and compile model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 10, input_length=X.shape[1]),\n",
        "    tf.keras.layers.LSTM(50),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Generate text based on seed\n",
        "def generate_text(seed_text, n_words=5):\n",
        "    for _ in range(n_words):\n",
        "        token_list = pad_sequences([tokenizer.texts_to_sequences([seed_text])[0]], maxlen=X.shape[1], padding='pre')\n",
        "        seed_text += ' ' + tokenizer.index_word[np.argmax(model.predict(token_list))]\n",
        "    return seed_text\n",
        "\n",
        "# Generate and print text\n",
        "print(generate_text(\"This is\", 5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vXjstPyqoeU",
        "outputId": "d7cd0d4b-9c8d-413b-9422-a5e48dfb0f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "This is the a review lot lot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample data\n",
        "corpus = [\n",
        "    \"This is a positive review\", \"I enjoyed the movie a lot\",\n",
        "    \"Great performance by the actors\", \"The plot was intriguing\",\n",
        "    \"Negative feedback about the service\", \"Disappointed with the product quality\",\n",
        "    \"Worst experience ever\"\n",
        "]\n",
        "\n",
        "# Step 1: Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1  # Count of unique words\n",
        "\n",
        "# Step 2: Create sequences for training\n",
        "sequences = []\n",
        "for text in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        sequences.append(token_list[:i+1])  # Creates incremental sequences\n",
        "\n",
        "# Step 3: Prepare X (inputs) and y (outputs) for the model\n",
        "X = pad_sequences([seq[:-1] for seq in sequences])  # Inputs (all except the last word)\n",
        "y = tf.keras.utils.to_categorical([seq[-1] for seq in sequences], num_classes=total_words)  # Output (last word)\n",
        "\n",
        "# Step 4: Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 10, input_length=X.shape[1]),  # Embedding layer for word representations\n",
        "    tf.keras.layers.LSTM(50),  # LSTM layer to learn sequence patterns\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')  # Output layer for word prediction\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "# Step 5: Function to generate text\n",
        "def generate_text(seed_text, num_words=5):\n",
        "    for _ in range(num_words):\n",
        "        token_list = pad_sequences([tokenizer.texts_to_sequences([seed_text])[0]], maxlen=X.shape[1])\n",
        "        predicted_word_index = np.argmax(model.predict(token_list), axis=-1)[0]\n",
        "        seed_text += ' ' + tokenizer.index_word[predicted_word_index]\n",
        "    return seed_text\n",
        "\n",
        "# Example usage\n",
        "print(generate_text(\"This is\", 5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loAVVrVw-PmJ",
        "outputId": "f3ec5a4e-07a3-4884-c1c9-e8cec6f1e0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "This is the a positive review review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Define a basic machine translation dataset\n",
        "source_texts = ['hello', 'how are you', 'goodbye']\n",
        "target_texts = ['bonjour', 'comment ça va', 'au revoir']\n",
        "\n",
        "# Create vocabulary and mapping from words to integers\n",
        "source_vocab = set(word_tokenize(\" \".join(source_texts)))\n",
        "target_vocab = set(word_tokenize(\" \".join(target_texts)))\n",
        "\n",
        "source_vocab_size = len(source_vocab) + 1  # +1 for padding\n",
        "target_vocab_size = len(target_vocab) + 1  # +1 for padding\n",
        "\n",
        "source_word_to_int = {word: idx+1 for idx, word in enumerate(source_vocab)}\n",
        "target_word_to_int = {word: idx+1 for idx, word in enumerate(target_vocab)}\n",
        "\n",
        "source_int_to_word = {idx + 1 : word for idx, word in enumerate(source_vocab)}\n",
        "target_int_to_word = {idx + 1: word for idx, word in enumerate(target_vocab)}\n",
        "\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "source_sequences = [[source_word_to_int[word] for word in word_tokenize(text)] for text in source_texts]\n",
        "target_sequences = [[target_word_to_int[word] for word in word_tokenize(text)] for text in target_texts]\n",
        "\n",
        "# Pad sequences to the same length\n",
        "max_sequence_length = max(len(seq) for seq in source_sequences)\n",
        "\n",
        "source_sequences = tf.keras.preprocessing.sequence.pad_sequences(source_sequences, maxlen=max_sequence_length, padding='post')\n",
        "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Build the model\n",
        "input_shape = (max_sequence_length, source_vocab_size)\n",
        "output_shape = (max_sequence_length, target_vocab_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(source_vocab_size, 64, input_length=max_sequence_length),\n",
        "    tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# One-hot encode the target sequences\n",
        "target_sequences_one_hot = np.array([tf.keras.utils.to_categorical(seq, num_classes=target_vocab_size) for seq in target_sequences])\n",
        "\n",
        "# Train the model\n",
        "model.fit(source_sequences, target_sequences_one_hot, epochs=30)\n",
        "\n",
        "# Translate a new input sequence\n",
        "input_sequence = \"how are you\"\n",
        "input_sequence = [source_word_to_int[word] for word in input_sequence.split()]\n",
        "input_sequence = tf.keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "output_sequence = model.predict(input_sequence)[0]\n",
        "# Decode the output sequence\n",
        "output_sequence = [target_int_to_word[np.argmax(word)] for word in output_sequence if np.argmax(word) != 0]\n",
        "print(\"Translated Sequence:\", ' '.join(output_sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mix68ikbrDIm",
        "outputId": "d9625746-8692-4b95-981b-2ea556d75825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.1111 - loss: 1.9331\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4444 - loss: 1.8867\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5556 - loss: 1.8405\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7778 - loss: 1.7938\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.7778 - loss: 1.7463\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7778 - loss: 1.6975\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.7778 - loss: 1.6469\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7778 - loss: 1.5946\n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7778 - loss: 1.5404\n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7778 - loss: 1.4846\n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.7778 - loss: 1.4275\n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7778 - loss: 1.3698\n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7778 - loss: 1.3124\n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7778 - loss: 1.2560\n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7778 - loss: 1.2016\n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7778 - loss: 1.1497\n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7778 - loss: 1.1005\n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7778 - loss: 1.0542\n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7778 - loss: 1.0104\n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7778 - loss: 0.9692\n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7778 - loss: 0.9301\n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7778 - loss: 0.8930\n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8889 - loss: 0.8575\n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8889 - loss: 0.8233\n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8889 - loss: 0.7902\n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.7578\n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.7262\n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.6953\n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.6651\n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.6358\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
            "Translated Sequence: comment ça va\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Define a basic dataset\n",
        "source_texts = ['hello', 'how are you', 'goodbye']\n",
        "target_texts = ['bonjour', 'comment ça va', 'au revoir']\n",
        "\n",
        "# Create vocabulary mappings\n",
        "source_vocab = {word: idx+1 for idx, word in enumerate(set(word_tokenize(\" \".join(source_texts))))}\n",
        "target_vocab = {word: idx+1 for idx, word in enumerate(set(word_tokenize(\" \".join(target_texts))))}\n",
        "source_sequences = [[source_vocab[word] for word in word_tokenize(text)] for text in source_texts]\n",
        "target_sequences = [[target_vocab[word] for word in word_tokenize(text)] for text in target_texts]\n",
        "\n",
        "# Pad sequences\n",
        "max_len = max(map(len, source_sequences))\n",
        "source_sequences = tf.keras.preprocessing.sequence.pad_sequences(source_sequences, maxlen=max_len, padding='post')\n",
        "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=max_len, padding='post')\n",
        "target_sequences_one_hot = np.array([tf.keras.utils.to_categorical(seq, num_classes=len(target_vocab)+1) for seq in target_sequences])\n",
        "\n",
        "# Model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(len(source_vocab) + 1, 64, input_length=max_len),\n",
        "    tf.keras.layers.SimpleRNN(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(len(target_vocab) + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(source_sequences, target_sequences_one_hot, epochs=30)\n",
        "\n",
        "# Translation\n",
        "input_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    [[source_vocab.get(word, 0) for word in word_tokenize(\"how are you\")]], maxlen=max_len, padding='post')\n",
        "output_seq = model.predict(input_seq)[0]\n",
        "translated = ' '.join([list(target_vocab.keys())[np.argmax(word)-1] for word in output_seq if np.argmax(word) != 0])\n",
        "print(\"Translated Sequence:\", translated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "W6fdI79I93Fc",
        "outputId": "db6bf803-99d5-4c6a-90f1-e8412081df73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9c5fbeee631>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create vocabulary mappings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msource_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtarget_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msource_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    }
  ]
}